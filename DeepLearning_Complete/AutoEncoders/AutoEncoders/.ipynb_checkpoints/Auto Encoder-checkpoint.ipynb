{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=pd.read_csv('movies.dat',sep='::',\n",
    "                  header=None,\n",
    "                  engine='python',\n",
    "                  encoding='latin-1'\n",
    "                  )\n",
    "users=pd.read_csv('users.dat',sep='::',\n",
    "                  header=None,\n",
    "                  engine='python',\n",
    "                  encoding='latin-1'\n",
    "                  )\n",
    "ratings=pd.read_csv('ratings.dat',sep='::',\n",
    "                  header=None,\n",
    "                  engine='python',\n",
    "                  encoding='latin-1'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                   1                             2\n",
       "0  1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1  2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2  3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3  4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4  5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1   2   3      4\n",
       "0  1  F   1  10  48067\n",
       "1  2  M  56  16  70072\n",
       "2  3  M  25  15  55117\n",
       "3  4  M  45   7  02460\n",
       "4  5  M  25  20  55455"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1  2          3\n",
       "0  1  1193  5  978300760\n",
       "1  1   661  3  978302109\n",
       "2  1   914  3  978301968\n",
       "3  1  3408  4  978300275\n",
       "4  1  2355  5  978824291"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3883, 3), (6040, 5), (3883, 3))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.shape,users.shape,movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\n",
       "2858    3428\n",
       "260     2991\n",
       "1196    2990\n",
       "1210    2883\n",
       "480     2672\n",
       "2028    2653\n",
       "589     2649\n",
       "2571    2590\n",
       "1270    2583\n",
       "593     2578\n",
       "Name: 2, dtype: int64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.groupby(1)[2].count().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(ratings[2])\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(users[1])\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set=pd.read_csv('u1.base',delimiter='\\t',names=['User','Movie','Rating','Timestamp'])\n",
    "training_set=np.array(training_set,dtype='int')\n",
    "test_set=pd.read_csv('u1.test',delimiter='\\t',names=['User','Movie','Rating','Timestamp'])\n",
    "test_set=np.array(test_set,dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[        1,         1,         5, 874965758],\n",
       "       [        1,         2,         3, 876893171],\n",
       "       [        1,         3,         4, 878542960],\n",
       "       ...,\n",
       "       [      943,      1188,         3, 888640250],\n",
       "       [      943,      1228,         3, 888640275],\n",
       "       [      943,      1330,         3, 888692465]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[        1,         6,         5, 887431973],\n",
       "       [        1,        10,         3, 875693118],\n",
       "       [        1,        12,         5, 878542960],\n",
       "       ...,\n",
       "       [      459,       934,         3, 879563639],\n",
       "       [      460,        10,         3, 882912371],\n",
       "       [      462,       682,         5, 886365231]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "943"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_users=max(max(training_set[:,0]),max(test_set[:,0]))\n",
    "nb_users=int(nb_users)\n",
    "nb_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1682"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_movies=max(max(training_set[:,1]),max(test_set[:,1]))\n",
    "nb_movies=int(nb_movies)\n",
    "nb_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert(df):\n",
    "#     a=pd.pivot_table(data=df,\n",
    "#                    index='User',\n",
    "#                    columns='Movie',\n",
    "#                    values='Rating'\n",
    "#                   )\n",
    "#     a.fillna(0.0,inplace=True)\n",
    "#     a=np.array(a)\n",
    "# #     a=a.values\n",
    "#     return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    new_data=[]\n",
    "    for id_users in range(1,nb_users+1):\n",
    "        id_movies=data[:,1][data[:,0]==id_users]\n",
    "        id_ratings=data[:,2][data[:,0]==id_users]\n",
    "        ratings=np.zeros(nb_movies)\n",
    "        ratings[id_movies-1]=id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set=convert(training_set)\n",
    "test_set=convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set=torch.FloatTensor(training_set)\n",
    "test_set=torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(SAE,self).__init__()\n",
    "        self.fc1=nn.Linear(nb_movies,20)\n",
    "        self.fc2=nn.Linear(20,10)\n",
    "        self.fc3=nn.Linear(10,20)\n",
    "        self.fc4=nn.Linear(20,nb_movies)\n",
    "        self.activation=nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        x=self.activation(self.fc1(x))\n",
    "        x=self.activation(self.fc2(x))\n",
    "        x=self.activation(self.fc3(x))\n",
    "        x=self.fc4(x)     \n",
    "        return x\n",
    "        \n",
    "sae=SAE()\n",
    "criterian=nn.MSELoss()\n",
    "optimizer=optim.RMSprop(sae.parameters(),lr=0.01,\n",
    "                       weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1loss: tensor(1.7717)\n",
      "epoch: 2loss: tensor(1.0967)\n",
      "epoch: 3loss: tensor(1.0533)\n",
      "epoch: 4loss: tensor(1.0384)\n",
      "epoch: 5loss: tensor(1.0308)\n",
      "epoch: 6loss: tensor(1.0265)\n",
      "epoch: 7loss: tensor(1.0237)\n",
      "epoch: 8loss: tensor(1.0219)\n",
      "epoch: 9loss: tensor(1.0206)\n",
      "epoch: 10loss: tensor(1.0196)\n",
      "epoch: 11loss: tensor(1.0188)\n",
      "epoch: 12loss: tensor(1.0183)\n",
      "epoch: 13loss: tensor(1.0180)\n",
      "epoch: 14loss: tensor(1.0174)\n",
      "epoch: 15loss: tensor(1.0173)\n",
      "epoch: 16loss: tensor(1.0169)\n",
      "epoch: 17loss: tensor(1.0166)\n",
      "epoch: 18loss: tensor(1.0163)\n",
      "epoch: 19loss: tensor(1.0163)\n",
      "epoch: 20loss: tensor(1.0162)\n",
      "epoch: 21loss: tensor(1.0161)\n",
      "epoch: 22loss: tensor(1.0162)\n",
      "epoch: 23loss: tensor(1.0158)\n",
      "epoch: 24loss: tensor(1.0158)\n",
      "epoch: 25loss: tensor(1.0158)\n",
      "epoch: 26loss: tensor(1.0156)\n",
      "epoch: 27loss: tensor(1.0152)\n",
      "epoch: 28loss: tensor(1.0150)\n",
      "epoch: 29loss: tensor(1.0129)\n",
      "epoch: 30loss: tensor(1.0115)\n",
      "epoch: 31loss: tensor(1.0096)\n",
      "epoch: 32loss: tensor(1.0089)\n",
      "epoch: 33loss: tensor(1.0051)\n",
      "epoch: 34loss: tensor(1.0045)\n",
      "epoch: 35loss: tensor(1.0013)\n",
      "epoch: 36loss: tensor(1.0002)\n",
      "epoch: 37loss: tensor(0.9978)\n",
      "epoch: 38loss: tensor(0.9967)\n",
      "epoch: 39loss: tensor(0.9929)\n",
      "epoch: 40loss: tensor(0.9918)\n",
      "epoch: 41loss: tensor(0.9921)\n",
      "epoch: 42loss: tensor(0.9890)\n",
      "epoch: 43loss: tensor(0.9859)\n",
      "epoch: 44loss: tensor(0.9866)\n",
      "epoch: 45loss: tensor(0.9847)\n",
      "epoch: 46loss: tensor(0.9815)\n",
      "epoch: 47loss: tensor(0.9794)\n",
      "epoch: 48loss: tensor(0.9810)\n",
      "epoch: 49loss: tensor(0.9758)\n",
      "epoch: 50loss: tensor(0.9749)\n",
      "epoch: 51loss: tensor(0.9696)\n",
      "epoch: 52loss: tensor(0.9709)\n",
      "epoch: 53loss: tensor(0.9706)\n",
      "epoch: 54loss: tensor(0.9751)\n",
      "epoch: 55loss: tensor(0.9784)\n",
      "epoch: 56loss: tensor(0.9765)\n",
      "epoch: 57loss: tensor(0.9721)\n",
      "epoch: 58loss: tensor(0.9702)\n",
      "epoch: 59loss: tensor(0.9675)\n",
      "epoch: 60loss: tensor(0.9679)\n",
      "epoch: 61loss: tensor(0.9637)\n",
      "epoch: 62loss: tensor(0.9659)\n",
      "epoch: 63loss: tensor(0.9631)\n",
      "epoch: 64loss: tensor(0.9628)\n",
      "epoch: 65loss: tensor(0.9597)\n",
      "epoch: 66loss: tensor(0.9587)\n",
      "epoch: 67loss: tensor(0.9564)\n",
      "epoch: 68loss: tensor(0.9566)\n",
      "epoch: 69loss: tensor(0.9560)\n",
      "epoch: 70loss: tensor(0.9558)\n",
      "epoch: 71loss: tensor(0.9540)\n",
      "epoch: 72loss: tensor(0.9521)\n",
      "epoch: 73loss: tensor(0.9512)\n",
      "epoch: 74loss: tensor(0.9515)\n",
      "epoch: 75loss: tensor(0.9481)\n",
      "epoch: 76loss: tensor(0.9568)\n",
      "epoch: 77loss: tensor(0.9536)\n",
      "epoch: 78loss: tensor(0.9546)\n",
      "epoch: 79loss: tensor(0.9518)\n",
      "epoch: 80loss: tensor(0.9523)\n",
      "epoch: 81loss: tensor(0.9492)\n",
      "epoch: 82loss: tensor(0.9517)\n",
      "epoch: 83loss: tensor(0.9489)\n",
      "epoch: 84loss: tensor(0.9508)\n",
      "epoch: 85loss: tensor(0.9491)\n",
      "epoch: 86loss: tensor(0.9487)\n",
      "epoch: 87loss: tensor(0.9481)\n",
      "epoch: 88loss: tensor(0.9447)\n",
      "epoch: 89loss: tensor(0.9436)\n",
      "epoch: 90loss: tensor(0.9442)\n",
      "epoch: 91loss: tensor(0.9420)\n",
      "epoch: 92loss: tensor(0.9430)\n",
      "epoch: 93loss: tensor(0.9414)\n",
      "epoch: 94loss: tensor(0.9524)\n",
      "epoch: 95loss: tensor(0.9542)\n",
      "epoch: 96loss: tensor(0.9554)\n",
      "epoch: 97loss: tensor(0.9518)\n",
      "epoch: 98loss: tensor(0.9497)\n",
      "epoch: 99loss: tensor(0.9505)\n",
      "epoch: 100loss: tensor(0.9536)\n",
      "epoch: 101loss: tensor(0.9488)\n",
      "epoch: 102loss: tensor(0.9482)\n",
      "epoch: 103loss: tensor(0.9438)\n",
      "epoch: 104loss: tensor(0.9443)\n",
      "epoch: 105loss: tensor(0.9407)\n",
      "epoch: 106loss: tensor(0.9420)\n",
      "epoch: 107loss: tensor(0.9391)\n",
      "epoch: 108loss: tensor(0.9416)\n",
      "epoch: 109loss: tensor(0.9385)\n",
      "epoch: 110loss: tensor(0.9416)\n",
      "epoch: 111loss: tensor(0.9381)\n",
      "epoch: 112loss: tensor(0.9388)\n",
      "epoch: 113loss: tensor(0.9385)\n",
      "epoch: 114loss: tensor(0.9387)\n",
      "epoch: 115loss: tensor(0.9365)\n",
      "epoch: 116loss: tensor(0.9396)\n",
      "epoch: 117loss: tensor(0.9356)\n",
      "epoch: 118loss: tensor(0.9369)\n",
      "epoch: 119loss: tensor(0.9356)\n",
      "epoch: 120loss: tensor(0.9360)\n",
      "epoch: 121loss: tensor(0.9349)\n",
      "epoch: 122loss: tensor(0.9343)\n",
      "epoch: 123loss: tensor(0.9329)\n",
      "epoch: 124loss: tensor(0.9337)\n",
      "epoch: 125loss: tensor(0.9319)\n",
      "epoch: 126loss: tensor(0.9325)\n",
      "epoch: 127loss: tensor(0.9312)\n",
      "epoch: 128loss: tensor(0.9320)\n",
      "epoch: 129loss: tensor(0.9308)\n",
      "epoch: 130loss: tensor(0.9307)\n",
      "epoch: 131loss: tensor(0.9295)\n",
      "epoch: 132loss: tensor(0.9304)\n",
      "epoch: 133loss: tensor(0.9287)\n",
      "epoch: 134loss: tensor(0.9304)\n",
      "epoch: 135loss: tensor(0.9359)\n",
      "epoch: 136loss: tensor(0.9384)\n",
      "epoch: 137loss: tensor(0.9373)\n",
      "epoch: 138loss: tensor(0.9353)\n",
      "epoch: 139loss: tensor(0.9369)\n",
      "epoch: 140loss: tensor(0.9360)\n",
      "epoch: 141loss: tensor(0.9324)\n",
      "epoch: 142loss: tensor(0.9326)\n",
      "epoch: 143loss: tensor(0.9317)\n",
      "epoch: 144loss: tensor(0.9302)\n",
      "epoch: 145loss: tensor(0.9301)\n",
      "epoch: 146loss: tensor(0.9296)\n",
      "epoch: 147loss: tensor(0.9282)\n",
      "epoch: 148loss: tensor(0.9287)\n",
      "epoch: 149loss: tensor(0.9281)\n",
      "epoch: 150loss: tensor(0.9277)\n",
      "epoch: 151loss: tensor(0.9267)\n",
      "epoch: 152loss: tensor(0.9270)\n",
      "epoch: 153loss: tensor(0.9255)\n",
      "epoch: 154loss: tensor(0.9259)\n",
      "epoch: 155loss: tensor(0.9254)\n",
      "epoch: 156loss: tensor(0.9267)\n",
      "epoch: 157loss: tensor(0.9249)\n",
      "epoch: 158loss: tensor(0.9249)\n",
      "epoch: 159loss: tensor(0.9244)\n",
      "epoch: 160loss: tensor(0.9250)\n",
      "epoch: 161loss: tensor(0.9244)\n",
      "epoch: 162loss: tensor(0.9244)\n",
      "epoch: 163loss: tensor(0.9232)\n",
      "epoch: 164loss: tensor(0.9234)\n",
      "epoch: 165loss: tensor(0.9228)\n",
      "epoch: 166loss: tensor(0.9232)\n",
      "epoch: 167loss: tensor(0.9225)\n",
      "epoch: 168loss: tensor(0.9236)\n",
      "epoch: 169loss: tensor(0.9226)\n",
      "epoch: 170loss: tensor(0.9228)\n",
      "epoch: 171loss: tensor(0.9220)\n",
      "epoch: 172loss: tensor(0.9222)\n",
      "epoch: 173loss: tensor(0.9215)\n",
      "epoch: 174loss: tensor(0.9218)\n",
      "epoch: 175loss: tensor(0.9217)\n",
      "epoch: 176loss: tensor(0.9217)\n",
      "epoch: 177loss: tensor(0.9210)\n",
      "epoch: 178loss: tensor(0.9212)\n",
      "epoch: 179loss: tensor(0.9203)\n",
      "epoch: 180loss: tensor(0.9210)\n",
      "epoch: 181loss: tensor(0.9205)\n",
      "epoch: 182loss: tensor(0.9208)\n",
      "epoch: 183loss: tensor(0.9203)\n",
      "epoch: 184loss: tensor(0.9201)\n",
      "epoch: 185loss: tensor(0.9200)\n",
      "epoch: 186loss: tensor(0.9212)\n",
      "epoch: 187loss: tensor(0.9197)\n",
      "epoch: 188loss: tensor(0.9205)\n",
      "epoch: 189loss: tensor(0.9198)\n",
      "epoch: 190loss: tensor(0.9193)\n",
      "epoch: 191loss: tensor(0.9194)\n",
      "epoch: 192loss: tensor(0.9187)\n",
      "epoch: 193loss: tensor(0.9189)\n",
      "epoch: 194loss: tensor(0.9191)\n",
      "epoch: 195loss: tensor(0.9188)\n",
      "epoch: 196loss: tensor(0.9191)\n",
      "epoch: 197loss: tensor(0.9179)\n",
      "epoch: 198loss: tensor(0.9181)\n",
      "epoch: 199loss: tensor(0.9180)\n",
      "epoch: 200loss: tensor(0.9182)\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterian(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data*mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step()\n",
    "    print('epoch: '+str(epoch)+'loss: '+ str(train_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,_),(X_test,_)=mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=(X_train/255).reshape(-1,784)\n",
    "X_test=(X_test/255).reshape(-1,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(z,d=False):\n",
    "    return Sigmoid(z)*(1-Sigmoid(z))+1e-12 if d else 1/(1+np.exp(-z))\n",
    "\n",
    "def Relu(z,d=False):\n",
    "    return (z>0)+1e-12 if d else z*(z>0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[{'act':Relu,'shape':(1024,784)},\n",
    "       {'act':Relu,'shape':(50,1024)},\n",
    "       {'act':Relu,'shape':(1024,50)},\n",
    "       {'act':Sigmoid,'shape':(784,1024)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "l,errors,epochs=len(layers),[],30\n",
    "\n",
    "lr,b1,b2=0.002,0.9,0.999\n",
    "rw,mw,rb,mb={},{},{},{}\n",
    "\n",
    "a,w,b,f={},{},{},{}\n",
    "\n",
    "for i,layer in zip(range(1,l+1),layers):\n",
    "    n_out,n_in=layer['shape']\n",
    "    f[i]=layer['act']\n",
    "    w[i]=np.random.randn(n_out,n_in)/n_in**0.5\n",
    "    b[i],rb[i],mb[i]=[np.zeros((n_out,1)) for i in [1,2,3]]\n",
    "    rw[i],mw[i]=[np.zeros((n_out,n_in)) for i in [1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss -  0.20146994635964727\n",
      "Val loss -  0.10551893234272341\n",
      "Val loss -  0.0849984311387851\n",
      "Val loss -  0.07729261587787317\n",
      "Val loss -  0.08499011565125816\n",
      "Val loss -  0.08608933612025674\n",
      "Val loss -  0.08093259439521068\n",
      "Val loss -  0.07358536744418732\n",
      "Val loss -  0.06905941007823008\n",
      "Val loss -  0.06814837683562769\n",
      "Val loss -  0.06463068375198901\n",
      "Val loss -  0.061040060285937114\n",
      "Val loss -  0.05950171925494192\n",
      "Val loss -  0.05841509826290356\n",
      "Val loss -  0.05666705027546306\n",
      "Val loss -  0.0554248703216699\n",
      "Val loss -  0.05320842892183775\n",
      "Val loss -  0.05147340139837516\n",
      "Val loss -  0.05027865482826964\n",
      "Val loss -  0.049176224829435365\n",
      "Val loss -  0.04824823142343551\n",
      "Val loss -  0.04713041606401685\n",
      "Val loss -  0.045749538385526034\n",
      "Val loss -  0.04426037112599704\n",
      "Val loss -  0.04330680999018104\n",
      "Val loss -  0.0421998568342713\n",
      "Val loss -  0.04161374854014044\n",
      "Val loss -  0.0406021149346513\n",
      "Val loss -  0.04005284394770222\n",
      "Val loss -  0.039219061599163184\n",
      "Val loss -  0.03875058472363023\n",
      "Val loss -  0.03824013140379076\n",
      "Val loss -  0.03764629550761387\n",
      "Val loss -  0.03721237122298704\n",
      "Val loss -  0.03673497951707539\n",
      "Val loss -  0.03624849537101397\n",
      "Val loss -  0.035811518943367746\n",
      "Val loss -  0.03542100216018945\n",
      "Val loss -  0.03498643107671919\n",
      "Val loss -  0.03445229411331585\n",
      "Val loss -  0.034053726913643424\n",
      "Val loss -  0.03359350317942437\n",
      "Val loss -  0.033245087188309465\n",
      "Val loss -  0.03291382548287852\n",
      "Val loss -  0.03245562808540567\n",
      "Val loss -  0.032215560071016704\n",
      "Val loss -  0.03183927023428497\n",
      "Val loss -  0.03154099188301635\n",
      "Val loss -  0.031033923482400524\n",
      "Val loss -  0.030693402911525006\n",
      "Val loss -  0.030419750719979623\n",
      "Val loss -  0.030190549582868292\n",
      "Val loss -  0.029884009551861063\n",
      "Val loss -  0.029392374584450667\n",
      "Val loss -  0.02907745421867167\n",
      "Val loss -  0.028770586896635007\n",
      "Val loss -  0.028600162202121127\n",
      "Val loss -  0.028164911322287333\n",
      "Val loss -  0.027820950537739624\n",
      "Val loss -  0.027721799715939105\n",
      "Val loss -  0.027305084900187584\n",
      "Val loss -  0.026919601450624536\n",
      "Val loss -  0.026668807310024252\n",
      "Val loss -  0.026455703022915316\n",
      "Val loss -  0.026200309931067686\n",
      "Val loss -  0.025914221397014346\n",
      "Val loss -  0.02571214380666328\n",
      "Val loss -  0.02579158141255282\n",
      "Val loss -  0.025728861004400617\n",
      "Val loss -  0.025172336465300437\n",
      "Val loss -  0.02487128826131472\n",
      "Val loss -  0.024704562395165962\n",
      "Val loss -  0.024558709122364644\n",
      "Val loss -  0.024134140559398075\n",
      "Val loss -  0.024059297760653072\n",
      "Val loss -  0.024070711822970815\n",
      "Val loss -  0.023611086348177653\n",
      "Val loss -  0.023313575510382555\n",
      "Val loss -  0.023207919210316164\n",
      "Val loss -  0.02309271976981327\n",
      "Val loss -  0.023264590769033023\n",
      "Val loss -  0.02260711920690625\n",
      "Val loss -  0.022407030502079896\n",
      "Val loss -  0.022469394299078106\n",
      "Val loss -  0.022160097692565273\n",
      "Val loss -  0.021925927319301423\n",
      "Val loss -  0.021837919028770452\n",
      "Val loss -  0.02167369394584595\n",
      "Val loss -  0.021642358958970497\n",
      "Val loss -  0.021314388073605757\n",
      "Val loss -  0.021180301510835477\n",
      "Val loss -  0.021088449083921827\n",
      "Val loss -  0.02094938607656715\n",
      "Val loss -  0.02080966223278949\n",
      "Val loss -  0.020678960206413258\n",
      "Val loss -  0.020559351166720585\n",
      "Val loss -  0.020476819957185463\n",
      "Val loss -  0.02051161956287369\n",
      "Val loss -  0.02027128750580651\n",
      "Val loss -  0.020147108282861287\n",
      "Val loss -  0.020076056139372092\n",
      "Val loss -  0.01990373255637648\n",
      "Val loss -  0.01976535742256938\n",
      "Val loss -  0.01975135465699492\n",
      "Val loss -  0.019603273128440245\n",
      "Val loss -  0.019505869843796594\n",
      "Val loss -  0.019323995499016318\n",
      "Val loss -  0.01930457259893982\n",
      "Val loss -  0.0191178191876926\n",
      "Val loss -  0.019004311863361954\n",
      "Val loss -  0.019086911781936538\n",
      "Val loss -  0.01882160463449283\n",
      "Val loss -  0.01878134695317608\n",
      "Val loss -  0.018682765972835504\n",
      "Val loss -  0.018546055316769786\n",
      "Val loss -  0.01849513044458606\n",
      "Val loss -  0.018489748512920116\n",
      "Val loss -  0.018360906038762823\n",
      "Val loss -  0.01840902723084165\n",
      "Val loss -  0.01812877082483227\n",
      "Val loss -  0.018095128369268573\n",
      "Val loss -  0.018118787398931354\n",
      "Val loss -  0.017913063415904536\n",
      "Val loss -  0.017857514457001056\n",
      "Val loss -  0.01783982228547189\n",
      "Val loss -  0.017734955521918734\n",
      "Val loss -  0.017640702905883585\n",
      "Val loss -  0.017799437690850902\n",
      "Val loss -  0.017522902059049814\n",
      "Val loss -  0.01747427320146278\n",
      "Val loss -  0.017613119480079178\n",
      "Val loss -  0.017301939700152643\n",
      "Val loss -  0.017242902360192387\n",
      "Val loss -  0.01743609102743504\n",
      "Val loss -  0.017126954338478485\n",
      "Val loss -  0.01710419623358287\n",
      "Val loss -  0.01702783317815314\n",
      "Val loss -  0.01693978947909858\n",
      "Val loss -  0.016856549492729737\n",
      "Val loss -  0.01689343402181355\n",
      "Val loss -  0.016770795431119166\n",
      "Val loss -  0.016671436388077256\n",
      "Val loss -  0.016817331666793226\n",
      "Val loss -  0.016556658915165896\n",
      "Val loss -  0.016493967672561196\n",
      "Val loss -  0.01655004183093394\n",
      "Val loss -  0.01640393637925057\n",
      "Val loss -  0.01637899185475681\n",
      "Val loss -  0.016500643425595832\n",
      "Val loss -  0.016216868060750823\n",
      "Val loss -  0.016204354087718018\n",
      "Val loss -  0.016327333103169986\n",
      "Val loss -  0.016106500343990877\n",
      "Val loss -  0.01605949258933959\n",
      "Val loss -  0.016113698728847985\n",
      "Val loss -  0.01603542433437515\n",
      "Val loss -  0.015916529299823703\n",
      "Val loss -  0.01614094898601872\n",
      "Val loss -  0.015830745294722063\n",
      "Val loss -  0.0157720252694951\n",
      "Val loss -  0.016012509060556762\n",
      "Val loss -  0.01569988805084615\n",
      "Val loss -  0.015633989552191568\n",
      "Val loss -  0.01586389031317195\n",
      "Val loss -  0.015514941426591931\n",
      "Val loss -  0.015499848305806162\n",
      "Val loss -  0.01554041280480278\n",
      "Val loss -  0.015378484772779234\n",
      "Val loss -  0.015380103095179804\n",
      "Val loss -  0.015390489166473486\n",
      "Val loss -  0.015320880883087386\n",
      "Val loss -  0.01526196442069466\n",
      "Val loss -  0.015272339642818688\n",
      "Val loss -  0.015216957370034364\n",
      "Val loss -  0.015113216213035725\n",
      "Val loss -  0.015058240439503942\n",
      "Val loss -  0.01520440305748324\n",
      "Val loss -  0.015064740071997634\n",
      "Val loss -  0.014963127186672032\n",
      "Val loss -  0.015109558922757699\n",
      "Val loss -  0.014893156990289967\n",
      "Val loss -  0.014817995493191422\n",
      "Val loss -  0.014954408292375914\n",
      "Val loss -  0.014765794145172215\n",
      "Val loss -  0.014731321091970506\n",
      "Val loss -  0.014735221651264195\n",
      "Val loss -  0.014670277314341973\n",
      "Val loss -  0.014605668547345898\n",
      "Val loss -  0.014623332414642589\n",
      "Val loss -  0.014518518893243827\n",
      "Val loss -  0.01451180647793877\n",
      "Val loss -  0.014487434317198945\n",
      "Val loss -  0.014412655103687127\n",
      "Val loss -  0.014411635800058612\n",
      "Val loss -  0.01436851782145857\n",
      "Val loss -  0.014403997861809095\n",
      "Val loss -  0.01430449155255958\n",
      "Val loss -  0.014239135667872606\n",
      "Val loss -  0.014248138057760167\n",
      "Val loss -  0.014183747557695305\n",
      "Val loss -  0.01418846375328558\n",
      "Val loss -  0.014127045040731315\n",
      "Val loss -  0.014084432485539014\n",
      "Val loss -  0.014068751337877215\n",
      "Val loss -  0.014005794846848926\n",
      "Val loss -  0.013982779963511898\n",
      "Val loss -  0.014004528419098277\n",
      "Val loss -  0.013927965016054084\n",
      "Val loss -  0.01392737333506683\n",
      "Val loss -  0.013884173194271732\n",
      "Val loss -  0.0138301518427158\n",
      "Val loss -  0.013836123607749573\n",
      "Val loss -  0.013797069669695227\n",
      "Val loss -  0.013774801382600168\n",
      "Val loss -  0.01374679357239207\n",
      "Val loss -  0.013721413848522504\n",
      "Val loss -  0.013710427031874218\n",
      "Val loss -  0.013700236488648656\n",
      "Val loss -  0.013626226521996626\n",
      "Val loss -  0.01359022478501147\n",
      "Val loss -  0.013648491949531421\n",
      "Val loss -  0.013568794995590633\n",
      "Val loss -  0.013514161727452663\n",
      "Val loss -  0.013533910068922059\n",
      "Val loss -  0.0134534682323964\n",
      "Val loss -  0.01352609613565025\n",
      "Val loss -  0.01340413877572891\n",
      "Val loss -  0.013361583095028452\n",
      "Val loss -  0.01338098848199864\n",
      "Val loss -  0.013337478748078366\n",
      "Val loss -  0.013317516142201573\n",
      "Val loss -  0.01327649069776914\n",
      "Val loss -  0.013280727715956918\n",
      "Val loss -  0.01323309678522648\n",
      "Val loss -  0.013193406726586445\n",
      "Val loss -  0.013186375831406767\n",
      "Val loss -  0.013188022763412714\n",
      "Val loss -  0.013147165525095384\n",
      "Val loss -  0.013151136169129028\n",
      "Val loss -  0.013093537351327194\n",
      "Val loss -  0.013065114158340541\n",
      "Val loss -  0.013100156465808765\n",
      "Val loss -  0.01303291214922635\n",
      "Val loss -  0.013020881747128373\n",
      "Val loss -  0.013022727873660635\n",
      "Val loss -  0.012993821728705604\n",
      "Val loss -  0.012975050315963382\n",
      "Val loss -  0.012990566968429349\n",
      "Val loss -  0.01290884288488269\n",
      "Val loss -  0.012879042253613637\n",
      "Val loss -  0.012936287922502118\n",
      "Val loss -  0.012882767854309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss -  0.012831993399892966\n",
      "Val loss -  0.012826480742948987\n",
      "Val loss -  0.012782067039379397\n",
      "Val loss -  0.01289415906819523\n",
      "Val loss -  0.012730733855133977\n",
      "Val loss -  0.01270286597626828\n",
      "Val loss -  0.012738576535562866\n",
      "Val loss -  0.012674433323347693\n",
      "Val loss -  0.012665398074209716\n",
      "Val loss -  0.012641711182395344\n",
      "Val loss -  0.012631006105665301\n",
      "Val loss -  0.012603545569045551\n",
      "Val loss -  0.012574706595813681\n",
      "Val loss -  0.0125657828344693\n",
      "Val loss -  0.012570195168791445\n",
      "Val loss -  0.012532126886256378\n",
      "Val loss -  0.012532217889405383\n",
      "Val loss -  0.0124942721547849\n",
      "Val loss -  0.012465694126754941\n",
      "Val loss -  0.01250056571669407\n",
      "Val loss -  0.01244495376300273\n",
      "Val loss -  0.012430864357259193\n",
      "Val loss -  0.012440221972149236\n",
      "Val loss -  0.012412465050309261\n",
      "Val loss -  0.012391581072867815\n",
      "Val loss -  0.012418020534491384\n",
      "Val loss -  0.012344842182194067\n",
      "Val loss -  0.012321885222258697\n",
      "Val loss -  0.012374598689981283\n",
      "Val loss -  0.012322486343310013\n",
      "Val loss -  0.012279070984727384\n",
      "Val loss -  0.012283424925335096\n",
      "Val loss -  0.012248522903607832\n",
      "Val loss -  0.012346277820541735\n",
      "Val loss -  0.012203112855378215\n",
      "Val loss -  0.012183485087650922\n",
      "Val loss -  0.01221094808557043\n",
      "Val loss -  0.012160462313276508\n",
      "Val loss -  0.012150518900725932\n",
      "Val loss -  0.012129529297070018\n",
      "Val loss -  0.01213277503117144\n",
      "Val loss -  0.012104233060970764\n",
      "Val loss -  0.012077896568219901\n",
      "Val loss -  0.012074035709291428\n",
      "Val loss -  0.012078537347313147\n",
      "Val loss -  0.012043788776873563\n",
      "Val loss -  0.012046525020239144\n",
      "Val loss -  0.012014880750318535\n",
      "Val loss -  0.01199149395233204\n",
      "Val loss -  0.012031391192520129\n",
      "Val loss -  0.01197385995068398\n",
      "Val loss -  0.011961109429422663\n",
      "Val loss -  0.011977395634929935\n",
      "Val loss -  0.011948826754349641\n",
      "Val loss -  0.011924631988215178\n",
      "Val loss -  0.011963681736183951\n",
      "Val loss -  0.011893065484416461\n",
      "Val loss -  0.011872035666262326\n",
      "Val loss -  0.011925156063875873\n",
      "Val loss -  0.011874760836675704\n",
      "Val loss -  0.01183365596681558\n",
      "Val loss -  0.011847082579005012\n",
      "Val loss -  0.01182003379390965\n",
      "Val loss -  0.011902818619452766\n",
      "Val loss -  0.011772383088227447\n",
      "Val loss -  0.011761772880772898\n",
      "Val loss -  0.01178409737220016\n",
      "Val loss -  0.011741116719691809\n",
      "Val loss -  0.011728935808873884\n",
      "Val loss -  0.011715739465469345\n",
      "Val loss -  0.011725312793393596\n",
      "Val loss -  0.011693606481071876\n",
      "Val loss -  0.01167407775423055\n",
      "Val loss -  0.011670027855522331\n",
      "Val loss -  0.011671834048310523\n",
      "Val loss -  0.01164093494235645\n",
      "Val loss -  0.011645084558465512\n",
      "Val loss -  0.011618408697226405\n",
      "Val loss -  0.011598693759050413\n",
      "Val loss -  0.011638939347770058\n",
      "Val loss -  0.01158431573224025\n",
      "Val loss -  0.011573800461632896\n",
      "Val loss -  0.01159037146896303\n",
      "Val loss -  0.011564990479650858\n",
      "Val loss -  0.011540722616691654\n",
      "Val loss -  0.011587272478157521\n",
      "Val loss -  0.011517061145249797\n",
      "Val loss -  0.011499157931016805\n",
      "Val loss -  0.01154953613702378\n",
      "Val loss -  0.011501203161430325\n",
      "Val loss -  0.011463046190672553\n",
      "Val loss -  0.011483762490894328\n",
      "Val loss -  0.01146226105568136\n",
      "Val loss -  0.01153176503051232\n",
      "Val loss -  0.011412727942263728\n",
      "Val loss -  0.01140768955693376\n",
      "Val loss -  0.011425121773679364\n",
      "Val loss -  0.011388835691346076\n",
      "Val loss -  0.011376027362309525\n",
      "Val loss -  0.011367184690158192\n",
      "Val loss -  0.011378770478967142\n",
      "Val loss -  0.011347538230033507\n",
      "Val loss -  0.011331716874710365\n",
      "Val loss -  0.011327622200894555\n",
      "Val loss -  0.0113295678224742\n",
      "Val loss -  0.011301428675510067\n",
      "Val loss -  0.011306740192240868\n",
      "Val loss -  0.011283901386024676\n",
      "Val loss -  0.011266490315808841\n",
      "Val loss -  0.01130759296714649\n",
      "Val loss -  0.011253656141105352\n",
      "Val loss -  0.011244859328902271\n",
      "Val loss -  0.011261415409092193\n",
      "Val loss -  0.011238179449236822\n",
      "Val loss -  0.011214243770733028\n",
      "Val loss -  0.011267590912752935\n",
      "Val loss -  0.011195888534224728\n",
      "Val loss -  0.011181442934838033\n",
      "Val loss -  0.01123032162100095\n",
      "Val loss -  0.011180077700146805\n",
      "Val loss -  0.011144277800354067\n",
      "Val loss -  0.011178649469738816\n",
      "Val loss -  0.011155639531469525\n",
      "Val loss -  0.011201897378877187\n",
      "Val loss -  0.011104285064733085\n",
      "Val loss -  0.011103864584718353\n",
      "Val loss -  0.011112322931046364\n",
      "Val loss -  0.01109065236348351\n",
      "Val loss -  0.011072724108366851\n",
      "Val loss -  0.011063866444900983\n",
      "Val loss -  0.011087771599510465\n",
      "Val loss -  0.011050128823782838\n",
      "Val loss -  0.011034775216997835\n",
      "Val loss -  0.011034141173809207\n",
      "Val loss -  0.011028497639889142\n",
      "Val loss -  0.011007280057507202\n",
      "Val loss -  0.011016521629484702\n",
      "Val loss -  0.010992396296798122\n",
      "Val loss -  0.010976685080921105\n",
      "Val loss -  0.011017840946200511\n",
      "Val loss -  0.010966923377045601\n",
      "Val loss -  0.010957976101913462\n",
      "Val loss -  0.0109739727223355\n",
      "Val loss -  0.0109547956308299\n",
      "Val loss -  0.010931103609666032\n",
      "Val loss -  0.010986361952302182\n",
      "Val loss -  0.010916428353361755\n",
      "Val loss -  0.010903466992273144\n",
      "Val loss -  0.010948678430036204\n",
      "Val loss -  0.01089900417312069\n",
      "Val loss -  0.01086634917833205\n",
      "Val loss -  0.010908273967179874\n",
      "Val loss -  0.01088359299696202\n",
      "Val loss -  0.010913882068783323\n",
      "Val loss -  0.010834048117187996\n",
      "Val loss -  0.010834568898473878\n",
      "Val loss -  0.010836180603148476\n",
      "Val loss -  0.01082821286476827\n",
      "Val loss -  0.010805538155656023\n",
      "Val loss -  0.01079371192606213\n",
      "Val loss -  0.010829793132366289\n",
      "Val loss -  0.010785749679065398\n",
      "Val loss -  0.010768012705415362\n",
      "Val loss -  0.010775172184376337\n",
      "Val loss -  0.010763287747211418\n",
      "Val loss -  0.010746668121932632\n",
      "Val loss -  0.01075867047115789\n",
      "Val loss -  0.010732773308987847\n",
      "Val loss -  0.010718377404511726\n",
      "Val loss -  0.01076050138763776\n",
      "Val loss -  0.010708898924337176\n",
      "Val loss -  0.010701345484222444\n",
      "Val loss -  0.010719061916569307\n",
      "Val loss -  0.010699210978067988\n",
      "Val loss -  0.010675504998495578\n",
      "Val loss -  0.01073748597800321\n",
      "Val loss -  0.010664683704646396\n",
      "Val loss -  0.01065351725121321\n",
      "Val loss -  0.01069716925953556\n",
      "Val loss -  0.010645892384721578\n",
      "Val loss -  0.010616751537780203\n",
      "Val loss -  0.010663465639046266\n",
      "Val loss -  0.01063555543180652\n",
      "Val loss -  0.010652661433904054\n",
      "Val loss -  0.01059090974062179\n",
      "Val loss -  0.010590105872100455\n",
      "Val loss -  0.010587146998492945\n",
      "Val loss -  0.010590407483249723\n",
      "Val loss -  0.010562336710716443\n",
      "Val loss -  0.010547698386039833\n",
      "Val loss -  0.010593538446846035\n",
      "Val loss -  0.010543728429719157\n",
      "Val loss -  0.010523389865686619\n",
      "Val loss -  0.010538531652864844\n",
      "Val loss -  0.010521420062906468\n",
      "Val loss -  0.010508040767260588\n",
      "Val loss -  0.010522428138046125\n",
      "Val loss -  0.010495686972373373\n",
      "Val loss -  0.010481234674057137\n",
      "Val loss -  0.010524532025381075\n",
      "Val loss -  0.010470899139144268\n",
      "Val loss -  0.01046707512904193\n",
      "Val loss -  0.010486244309605874\n",
      "Val loss -  0.010462315200021992\n",
      "Val loss -  0.0104406661368126\n",
      "Val loss -  0.010510946240267544\n",
      "Val loss -  0.010432746835994785\n",
      "Val loss -  0.010426049103630554\n",
      "Val loss -  0.010470946220375147\n",
      "Val loss -  0.010413578098690224\n",
      "Val loss -  0.010388021085252911\n",
      "Val loss -  0.010440976958650709\n",
      "Val loss -  0.010404563444865107\n",
      "Val loss -  0.010408425360114094\n",
      "Val loss -  0.010368354475000256\n",
      "Val loss -  0.01036362109535759\n",
      "Val loss -  0.010356739150075591\n",
      "Val loss -  0.010372567071413957\n",
      "Val loss -  0.010336485331349605\n",
      "Val loss -  0.010319928093981072\n",
      "Val loss -  0.010376991023810251\n",
      "Val loss -  0.010318188660498707\n",
      "Val loss -  0.010297122313506683\n",
      "Val loss -  0.010319591386621273\n",
      "Val loss -  0.01029634254549799\n",
      "Val loss -  0.010287884393092079\n",
      "Val loss -  0.010303402600526179\n",
      "Val loss -  0.010274709278640298\n",
      "Val loss -  0.01026220006017774\n",
      "Val loss -  0.0103088097699087\n",
      "Val loss -  0.010249993246908302\n",
      "Val loss -  0.01024970816355434\n",
      "Val loss -  0.010270765845716961\n",
      "Val loss -  0.010240973637399911\n",
      "Val loss -  0.010222288889515077\n",
      "Val loss -  0.010296209968999681\n",
      "Val loss -  0.01021487349814939\n",
      "Val loss -  0.010216286881552286\n",
      "Val loss -  0.010259513037474575\n",
      "Val loss -  0.01019559510204057\n",
      "Val loss -  0.010177726421344253\n",
      "Val loss -  0.010233431516703785\n",
      "Val loss -  0.010186816764092332\n",
      "Val loss -  0.010181308390296902\n",
      "Val loss -  0.010163832078747664\n",
      "Val loss -  0.010151769102858078\n",
      "Val loss -  0.010143952711882155\n",
      "Val loss -  0.010171791759673782\n",
      "Val loss -  0.010125528501085905\n",
      "Val loss -  0.01010937941462485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss -  0.010178334152404399\n",
      "Val loss -  0.010107521695072761\n",
      "Val loss -  0.01008697314893679\n",
      "Val loss -  0.01012002265331534\n",
      "Val loss -  0.01009023509701415\n",
      "Val loss -  0.010088994955558182\n",
      "Val loss -  0.01010321108146353\n",
      "Val loss -  0.010070809768101104\n",
      "Val loss -  0.010064175377144121\n",
      "Val loss -  0.010112645953184935\n",
      "Val loss -  0.010045332201515291\n",
      "Val loss -  0.0100540489232979\n",
      "Val loss -  0.010073616549083394\n",
      "Val loss -  0.010032854139411669\n",
      "Val loss -  0.010023595862077106\n",
      "Val loss -  0.010102323417999055\n",
      "Val loss -  0.010013382108534214\n",
      "Val loss -  0.010032082382317902\n",
      "Val loss -  0.01006749942525599\n",
      "Val loss -  0.009991997617912638\n",
      "Val loss -  0.009992471207908344\n",
      "Val loss -  0.010043974595961156\n",
      "Val loss -  0.009978866295047444\n",
      "Val loss -  0.009970190635718728\n",
      "Val loss -  0.009978350974255985\n",
      "Val loss -  0.009952393238875945\n",
      "Val loss -  0.009954697614493848\n",
      "Val loss -  0.009993607225497388\n",
      "Val loss -  0.009930254175504707\n",
      "Val loss -  0.009923463900235022\n",
      "Val loss -  0.00999853367852169\n",
      "Val loss -  0.009910426354759758\n",
      "Val loss -  0.009902841064683709\n",
      "Val loss -  0.00994058140589882\n",
      "Val loss -  0.00990977914200287\n",
      "Val loss -  0.00992676163738366\n",
      "Val loss -  0.009917437784563016\n",
      "Val loss -  0.009898082453067737\n",
      "Val loss -  0.00990291932241674\n",
      "Val loss -  0.009923995690473034\n",
      "Val loss -  0.00986577464837874\n",
      "Val loss -  0.009894722072773246\n",
      "Val loss -  0.009881583914593362\n",
      "Val loss -  0.009841672819007942\n",
      "Val loss -  0.009864853229963225\n",
      "Val loss -  0.009907022679739402\n",
      "Val loss -  0.009843113112545938\n",
      "Val loss -  0.009904935819328088\n",
      "Val loss -  0.009872568714245935\n",
      "Val loss -  0.009819043695523746\n",
      "Val loss -  0.009867165419937563\n",
      "Val loss -  0.00984311994245016\n",
      "Val loss -  0.009796064387325828\n",
      "Val loss -  0.009789779995815802\n",
      "Val loss -  0.009787622045047548\n",
      "Val loss -  0.00978679407123531\n",
      "Val loss -  0.009823883081515676\n",
      "Val loss -  0.00980046681966076\n",
      "Val loss -  0.009795003575081708\n",
      "Val loss -  0.009797383850010659\n",
      "Val loss -  0.009784507575987051\n",
      "Val loss -  0.009783422554444622\n",
      "Val loss -  0.009794977609245492\n",
      "Val loss -  0.009740696598200262\n",
      "Val loss -  0.00986204050149724\n",
      "Val loss -  0.00985686177850186\n",
      "Val loss -  0.009714886929253059\n",
      "Val loss -  0.00991635166213374\n",
      "Val loss -  0.009801887516112886\n",
      "Val loss -  0.009692619087242015\n",
      "Val loss -  0.009900016720458031\n",
      "Val loss -  0.00976785685230616\n",
      "Val loss -  0.009714082541627252\n",
      "Val loss -  0.009856847102729091\n",
      "Val loss -  0.009723127220269813\n",
      "Val loss -  0.009680984849118713\n",
      "Val loss -  0.009875033429737088\n",
      "Val loss -  0.009717866514071374\n",
      "Val loss -  0.009690265270143428\n",
      "Val loss -  0.009857434261870683\n",
      "Val loss -  0.009650730739833637\n",
      "Val loss -  0.009700271223985613\n",
      "Val loss -  0.009751498334866989\n",
      "Val loss -  0.009688460388539056\n",
      "Val loss -  0.009753340071028132\n",
      "Val loss -  0.00962445000317512\n"
     ]
    }
   ],
   "source": [
    "for t in range(1, epochs+1):\n",
    "    for batch in np.split(X_train, 30):\n",
    "        # Forward pass\n",
    "        a[0] = batch.T\n",
    "        for i in range(1,l+1):\n",
    "            a[i] = f[i]((w[i] @ a[i-1]) + b[i])\n",
    "        # Backpropagation\n",
    "        dz,dw,db = {},{},{}\n",
    "        for i in range(1,l+1)[::-1]:\n",
    "            d = w[i+1].T @ dz[i+1] if l-i else 0.5*(a[l]-a[0])\n",
    "            dz[i] = d * f[i](a[i],d=1)\n",
    "            dw[i] = dz[i] @ a[i-1].T\n",
    "            db[i] = np.sum(dz[i], 1, keepdims=True)\n",
    "        # Adam updates\n",
    "        def adam(m, r, z, dz, i):\n",
    "            m[i] = b1 * m[i] + (1 - b1) * dz[i]\n",
    "            r[i] = b2 * r[i] + (1 - b2) * dz[i]**2\n",
    "            m_hat = m[i] / (1. - b1**t)\n",
    "            r_hat = r[i] / (1. - b2**t) \n",
    "            z[i] -= lr * m_hat / (r_hat**0.5 + 1e-12)\n",
    "        for i in range(1,l+1):\n",
    "            adam(mw, rw, w, dw, i)\n",
    "            adam(mb, rb, b, db, i)\n",
    "    # Validate\n",
    "        a[0] = X_test.T\n",
    "        for i in range(1,l+1):\n",
    "            a[i] = f[i]((w[i] @ a[i-1]) + b[i])\n",
    "        errors += [np.mean((a[l]-a[0])**2)]\n",
    "        print(\"Val loss - \", errors[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "a[0] = X_train[:20].T\n",
    "#forward pass\n",
    "for i in range(1,l+1):\n",
    "    a[i] = f[i](w[i] @ a[i-1] + b[i])\n",
    "y_pred = a[l]\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "for i in range(20):\n",
    "    plt.subplot(3, 20, i + 1)\n",
    "    plt.imshow(X_train[i].reshape(28,28), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.grid(b=False)\n",
    "\n",
    "for i in range(20):\n",
    "    plt.subplot(3, 20, i + 1 + 20)\n",
    "    plt.imshow(a[l-2].T[i].reshape(5,-1), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.grid(b=False)\n",
    "    \n",
    "for i in range(20):\n",
    "    plt.subplot(3, 20, i + 1 + 40)\n",
    "    plt.imshow(y_pred.T[i].reshape(28,28), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.grid(b=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
